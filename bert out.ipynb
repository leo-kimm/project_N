{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0de44c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SKT Brain github 주소는 다음과 같습니다. https://github.com/SKTBrain/KoBERT\n",
    "\n",
    "# !pip install mxnet\n",
    "# !pip install gluonnlp pandas tqdm\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers==3 # 최신 버전으로 설치하면 \"Input: must be Tensor, not str\" 라는 에러 발생\n",
    "# !pip install torch\n",
    "\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# kobert \n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "# transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96dc11e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 시\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Bert모델, Voca 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18170fb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습용 데이터셋 불러오기\n",
    "import pandas as pd\n",
    "# !wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
    "\n",
    "# !wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582c08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#field_indices=[데이터 있는 열, 레이블링 된 열]\n",
    "#num_discard_samples=헤더가 있으면 상위 row 1개 제거\n",
    "# df_train = nlp.data.TSVDataset(\"ratings_train.txt?dl=1\", field_indices=[1,2],num_discard_samples=1)\n",
    "# df_test = nlp.data.TSVDataset(\"ratings_test.txt?dl=1\", field_indices=[1,2],num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563ea85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759bb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 질병명 라벨링\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(new_data['질병명'])\n",
    "# new_data['질병명'] = encoder.transform(new_data['질병명'])\n",
    "# new_data.head()\n",
    "\n",
    "# # 라벨링된 질병명 매핑 ex) {0: '비염', 1: '소화불량', 2: '수족냉증', 3: '식중독', 4: '질염'}\n",
    "# mapping = dict(zip(range(len(encoder.classes_)), encoder.classes_))\n",
    "# mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9904651",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../challenge/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8aed84ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>김은별 일 현지시간 유럽 주요 증시는 하락 출발했다 개장 직후 영국 FTSE 지수...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>전작 신뢰에 양호한 수익률로 자금 유입 활발 KB중소형주포커스 올 억 자금몰이 형만...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CJ오쇼핑 GS홈쇼핑 하이트진로 롯데삼강 포스코ICT 에스원 등이 지난해 사상 처음...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>LG생활건강이 수익성을 회복할지 주목된다 일 금융투자업계에 따르면 LG생활건강의 올...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>한화 가 신정부 관련주로 뜨고 있다 그룹 내 주력사업인 방위산업과 항공우주산업 보험...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>유럽증시가 지난달 일 현지시간 이틀 연속 상승세를 이어갔다 이탈리아 총선 결과 충격...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>태블릿 PC 등 활용한 아웃도어 세일즈 잇달아 신시장 개척 경비절감 효과 국내 증권...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>미국 연방 정부의 예산이 자동으로 삭감되는 이른바 시퀘스터 sequester 가 결...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>코웨이 솔고바이오 등 월에 상승 봄철 계절 테마주라 할 수 있는 황사 관련주들이 올...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>헤럴드생생뉴스 미국 민주당과 공화당이 시퀘스터 회피를 위해 제출안 법안이 모두 부결...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  cls\n",
       "29    김은별 일 현지시간 유럽 주요 증시는 하락 출발했다 개장 직후 영국 FTSE 지수...    0\n",
       "33   전작 신뢰에 양호한 수익률로 자금 유입 활발 KB중소형주포커스 올 억 자금몰이 형만...    1\n",
       "25   CJ오쇼핑 GS홈쇼핑 하이트진로 롯데삼강 포스코ICT 에스원 등이 지난해 사상 처음...    1\n",
       "48   LG생활건강이 수익성을 회복할지 주목된다 일 금융투자업계에 따르면 LG생활건강의 올...    1\n",
       "88   한화 가 신정부 관련주로 뜨고 있다 그룹 내 주력사업인 방위산업과 항공우주산업 보험...    1\n",
       "122  유럽증시가 지난달 일 현지시간 이틀 연속 상승세를 이어갔다 이탈리아 총선 결과 충격...    1\n",
       "32   태블릿 PC 등 활용한 아웃도어 세일즈 잇달아 신시장 개척 경비절감 효과 국내 증권...    1\n",
       "68   미국 연방 정부의 예산이 자동으로 삭감되는 이른바 시퀘스터 sequester 가 결...    1\n",
       "16   코웨이 솔고바이오 등 월에 상승 봄철 계절 테마주라 할 수 있는 황사 관련주들이 올...    0\n",
       "43   헤럴드생생뉴스 미국 민주당과 공화당이 시퀘스터 회피를 위해 제출안 법안이 모두 부결...    1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.read_csv(PATH + \"Clean_train.csv\", encoding='euc-kr')\n",
    "new_data.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d211825",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for q, label in zip(new_data['title'], new_data['cls'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edaf6e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['뉴욕 이상원 특파원 일 현지시간 뉴욕증시는 미국의 경제 지표 부진과 연방 정부의 예산 자동 삭감 우려에 하락세로 출발했다 이날 오전 시 분 현재 뉴욕증권거래소 NYSE 에서 다우존스 산업평균지수는 전날보다 포인트 떨어진 에서 거래되고 있다 스탠더드 앤드 푸어스 S P 지수는 포인트 낮은 을 나스닥 종합지수는 포인트 내려간 을 각각 기록하고 있다 시장은 미국 연방 정부의 예산 자동 삭감을 일컫는 시퀘스터 협상에 주목하고 있다 버락 오바마 대통령은 시퀘스터가 발동하는 이날 존 베이너 공화당 하원 의장 등 의회 지도부와 시퀘스터 돌파구 마련을 위한 회의를 한다 미국 소비자들의 개인소득은 비교적 큰 폭으로 줄었으나 지출은 증가세를 이어갔다 미국 상무부는 지난 월 소비지출이 전월보다 늘어나 지난해 월 이후 개월 연속 증가했다고 밝혔다 이는 시장전문가들의 예상치 평균과 일치하는 증가 폭이다 그러나 같은 달 개인소득은 줄어들어 지난 년 월 이후 년 만에 최대 감소폭을 기록했다 뉴욕상업거래소 NYMEX 에서 서부텍사스산 원유 WTI 는 전날보다 달러 떨어진 배럴당 달러 선에서 움직이고 있다 ', '0']\n",
      "[' 김재연 중국의 월 HSBC 제조업 구매관리자지수 PMI 수정치가 를 기록했다고 블룸버그통신이 일 보도했다 이는 시장 예상치 는 물론 전월치 을 하회한 것이다 ', '0']\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0])\n",
    "print(data_list[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d61d0b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea60cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: 99\n",
      "test shape is: 25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train / Test set 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "print(\"train shape is:\", len(dataset_train))\n",
    "print(\"test shape is:\", len(dataset_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa82f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kobert 입력 데이터로 만들기\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddc8d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 10 # 훈련 반복횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0d2f315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "# 기본 Bert tokenizer 사용\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28b6dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 패딩화\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5353882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2, 1146, 7096, 5012, 7645, 6705, 5136, 7207, 5379, 6116,  517,\n",
       "        7802, 2277, 6903, 2332, 1235, 5439, 7087, 6896, 3468, 7941, 7207,\n",
       "        7003,  517, 5566, 5436, 7227, 7088, 1938, 2280, 4555, 7295, 7139,\n",
       "        3803, 2357, 5760, 1146, 5566, 7095, 1235, 5439, 7087, 5859, 3468,\n",
       "        7941, 5436, 7227, 7088, 2779, 5330,  742, 5760, 1222, 7253, 7846,\n",
       "        7088, 1614, 1146, 6335, 3004, 6077, 3811,  851,    3], dtype=int32),\n",
       " array(64, dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       dtype=int32),\n",
       " 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]\n",
    "#첫 번째는 패딩된 시퀀스\n",
    "#두 번째는 길이와 타입에 대한 내용\n",
    "#세 번재는 어텐션 마스크 시퀀스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "992041c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch 형식의 dataset 만들어주기 (pytorch용 DataLoader 사용)\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "768c6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kobert 학습모델 만들기\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 2, # softmax 사용 <- binary일 경우는 2\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66a65790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision cudatoolkit=10.1 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7bc1ab0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f47b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert 모델 불러오기\n",
    "\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "\n",
    "# optimizer설정 하고 schedule 설정(linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# 옵티마이저 선언\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd2045cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7eff5c213f28>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "  \n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f4dc9cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb14842643448d1807194d992d5db88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7736221551895142 train acc 0.46875\n",
      "epoch 1 train acc 0.5200892857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bab9ffe41fb4189b5eeb81123d5fb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada408dc87d3436eb10f135d438874c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.674468457698822 train acc 0.5625\n",
      "epoch 2 train acc 0.6526785714285714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3998e5ed23504eb991dd1d7a69ea7305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b958efd5d324257830c5b99d6c74f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.6645175218582153 train acc 0.5625\n",
      "epoch 3 train acc 0.6955357142857144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c42f69b84a243de98bd8bf67e479443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292f92868b514b8da97d54858e82cd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.5460506677627563 train acc 0.765625\n",
      "epoch 4 train acc 0.8113839285714286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2cfb62846c466eb996339357a292a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730b6c4ca62f472bb01d2638e0423cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.44281619787216187 train acc 0.84375\n",
      "epoch 5 train acc 0.8933035714285714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9497cf9951774fad80967c8565644f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e9d19bbc014f82b9917721fa993156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.32873672246932983 train acc 0.890625\n",
      "epoch 6 train acc 0.9167410714285714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b923f6b438db4ac2b8e7e13772ce8fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.72\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad994f381d24955914a1aa325ee10e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.276432067155838 train acc 0.921875\n",
      "epoch 7 train acc 0.9466517857142858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58962526e75a4482981e893c5f79e1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 test acc 0.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8869525c13b94a22a360a1fffda477dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 0.2501513957977295 train acc 0.953125\n",
      "epoch 8 train acc 0.9622767857142858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a9c9fd685b44d3b9b61ff19812bc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 test acc 0.84\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7455839de08046cfbacb00bd7b7a9248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 batch id 1 loss 0.21101213991641998 train acc 0.96875\n",
      "epoch 9 train acc 0.9700892857142858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335960ddb37548c9bbdb0bb50b014590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 test acc 0.84\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7f36d5965c4fe78df17bc29c9616f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 batch id 1 loss 0.22332027554512024 train acc 0.953125\n",
      "epoch 10 train acc 0.9622767857142858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63be0d417734bc1aaa9b7a6ec35240e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 test acc 0.84\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 시작\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5d10466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c694ab186b4248ae9f24a2a6341cee41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8443,  1.2456]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9b16f30c8d41188153c6cdb568dc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 test acc 2.52\n"
     ]
    }
   ],
   "source": [
    "# 테스트 문장 예측\n",
    "test_sentence = \"본격적으로 막이 오른 쌍용차 인수전이 SM그룹의 '깜짝 등판'으로 후끈 달아오른 가운데 당사자인 쌍용차 역시 예상외의 흥행에 미소 지으며 후속 작업을 준비하고 있다. 최근 쌍용차가 친환경차 전용공장 건설 계획을 밝히는 등 전기차 전환에 뒤늦게 속도를 내는 가운데 상당수의 투자자가 전기차 사업 확대를 목표로 인수 의향을 밝히고 나선 것도 긍정적이라는 분위기다.\"\n",
    "test_label = 1 # 실제 값\n",
    "\n",
    "unseen_test = pd.DataFrame([[test_sentence, test_label]], columns = [['title', 'cls']])\n",
    "unseen_values = unseen_test.values\n",
    "test_set = BERTDataset(unseen_values, 0, 1, tok, max_len, True, False)\n",
    "test_input = torch.utils.data.DataLoader(test_set, batch_size=1, num_workers=5)\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_input)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    print(out)\n",
    "    \n",
    "model.eval() # 평가 모드로 변경\n",
    "    \n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성값 예측하는 함수 만들기\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 위에서 설정한 tok, max_len, batch_size, device를 그대로 입력\n",
    "# comment : 예측하고자 하는 텍스트 데이터 리스트\n",
    "def getSentimentValue(comment, tok, max_len, batch_size, device):\n",
    "    commnetslist = [] # 텍스트 데이터를 담을 리스트\n",
    "    emo_list = [] # 감성 값을 담을 리스트\n",
    "    for c in comment: # 모든 댓글\n",
    "        commnetslist.append( [c, 5] ) # [댓글, 임의의 양의 정수값] 설정\n",
    "\n",
    "    pdData = pd.DataFrame( commnetslist, columns = [['댓글', '감성']] )\n",
    "    pdData = pdData.values\n",
    "    test_set = BERTDataset(pdData, 0, 1, tok, max_len, True, False) \n",
    "    test_input = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_input):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length \n",
    "        # 이때, out이 예측 결과 리스트\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        # e는 2가지 실수 값으로 구성된 리스트\n",
    "        # 0번 인덱스가 더 크면 부정, 긍정은 반대\n",
    "        for e in out:\n",
    "            if e[0]>e[1]: # 부정\n",
    "            value = 0\n",
    "            else: #긍정\n",
    "            value = 1\n",
    "            emo_list.append(value)\n",
    "\n",
    "    return emo_list # 텍스트 데이터에 1대1 매칭되는 감성값 리스트 반환\n",
    "\n",
    "# input : 텍스트 데이터 리스트 외 KoBERT 설정 파라미터들\n",
    "# output : 입력한 텍스트 데이터 리스트와 1대 1 매칭 되는 감성 값 리스트"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py36_pytorch",
   "language": "python",
   "name": "conda-env-azureml_py36_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
