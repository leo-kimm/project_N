{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec0de44c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Downloading mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 46.9 MB 120 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from mxnet) (2.25.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from mxnet) (1.19.5)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (1.26.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (4.0.0)\n",
      "Installing collected packages: graphviz, mxnet\n",
      "  Attempting uninstall: graphviz\n",
      "    Found existing installation: graphviz 0.16\n",
      "    Uninstalling graphviz-0.16:\n",
      "      Successfully uninstalled graphviz-0.16\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.8.0.post0\n",
      "Collecting gluonnlp\n",
      "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
      "\u001b[K     |████████████████████████████████| 344 kB 29.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (4.61.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from gluonnlp) (1.19.5)\n",
      "Requirement already satisfied: cython in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from gluonnlp) (0.29.23)\n",
      "Requirement already satisfied: packaging in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from gluonnlp) (20.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Building wheels for collected packages: gluonnlp\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=468329 sha256=d6f817436ff3059fff30fe904cb8d55ebe772b00f4a7826aadfa7b2f15904d95\n",
      "  Stored in directory: /home/adminuser/.cache/pip/wheels/62/62/9a/53be069ac8c9dde533dacce0e716193a4a43e87b5d37f5008c\n",
      "Successfully built gluonnlp\n",
      "Installing collected packages: gluonnlp\n",
      "Successfully installed gluonnlp-0.10.0\n",
      "Requirement already satisfied: sentencepiece in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (0.1.95)\n",
      "Collecting transformers==3\n",
      "  Downloading transformers-3.0.0-py3-none-any.whl (754 kB)\n",
      "\u001b[K     |████████████████████████████████| 754 kB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from transformers==3) (0.1.95)\n",
      "Requirement already satisfied: packaging in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from transformers==3) (20.9)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from transformers==3) (0.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from transformers==3) (4.61.0)\n",
      "Requirement already satisfied: numpy in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from transformers==3) (1.19.5)\n",
      "Requirement already satisfied: sacremoses in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from transformers==3) (0.0.45)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from transformers==3) (3.0.12)\n",
      "Collecting tokenizers==0.8.0-rc4\n",
      "  Downloading tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 82.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from transformers==3) (2021.4.4)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from transformers==3) (2.25.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from packaging->transformers==3) (2.4.7)\n",
      "Requirement already satisfied: six in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from sacremoses->transformers==3) (1.15.0)\n",
      "Requirement already satisfied: joblib in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from sacremoses->transformers==3) (0.14.1)\n",
      "Requirement already satisfied: click in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from sacremoses->transformers==3) (8.0.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from requests->transformers==3) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from requests->transformers==3) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from requests->transformers==3) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from requests->transformers==3) (1.26.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from click->sacremoses->transformers==3) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->click->sacremoses->transformers==3) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->click->sacremoses->transformers==3) (3.7.4.3)\n",
      "\u001b[31mERROR: azureml-automl-dnn-nlp 1.29.0 has requirement transformers<=4.5.1,>=4.1.0, but you'll have transformers 3.0.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.2\n",
      "    Uninstalling tokenizers-0.10.2:\n",
      "      Successfully uninstalled tokenizers-0.10.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.5.1\n",
      "    Uninstalling transformers-4.5.1:\n",
      "      Successfully uninstalled transformers-4.5.1\n",
      "Successfully installed tokenizers-0.8.0rc4 transformers-3.0.0\n",
      "Requirement already satisfied: torch in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (1.8.1)\n",
      "Requirement already satisfied: typing_extensions in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from torch) (1.19.5)\n",
      "Requirement already satisfied: dataclasses in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-zpfwm4j1\n",
      "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-zpfwm4j1\n",
      "Building wheels for collected packages: kobert\n",
      "  Building wheel for kobert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobert: filename=kobert-0.1.2-py3-none-any.whl size=12761 sha256=1a84c970364cc71cf97286e2d035bfbe1ada88852fca1aa7545d3abb7e6c67f2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hxm_0s8w/wheels/4b/9b/3f/583189713d96fda0f291ca1f228ab0e0c981641dcdaf7c7cdf\n",
      "Successfully built kobert\n",
      "Installing collected packages: kobert\n",
      "Successfully installed kobert-0.1.2\n"
     ]
    }
   ],
   "source": [
    "# SKT Brain github 주소는 다음과 같습니다. https://github.com/SKTBrain/KoBERT\n",
    "\n",
    "# !pip install mxnet\n",
    "# !pip install gluonnlp pandas tqdm\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers==3 # 최신 버전으로 설치하면 \"Input: must be Tensor, not str\" 라는 에러 발생\n",
    "# !pip install torch\n",
    "\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# kobert \n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "# transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96dc11e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████]\n",
      "[██████████████████████████████████████████████████]\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 시\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Bert모델, Voca 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18170fb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습용 데이터셋 불러오기\n",
    "import pandas as pd\n",
    "# !wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
    "\n",
    "# !wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582c08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#field_indices=[데이터 있는 열, 레이블링 된 열]\n",
    "#num_discard_samples=헤더가 있으면 상위 row 1개 제거\n",
    "# df_train = nlp.data.TSVDataset(\"ratings_train.txt?dl=1\", field_indices=[1,2],num_discard_samples=1)\n",
    "# df_test = nlp.data.TSVDataset(\"ratings_test.txt?dl=1\", field_indices=[1,2],num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563ea85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759bb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 질병명 라벨링\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(new_data['질병명'])\n",
    "# new_data['질병명'] = encoder.transform(new_data['질병명'])\n",
    "# new_data.head()\n",
    "\n",
    "# # 라벨링된 질병명 매핑 ex) {0: '비염', 1: '소화불량', 2: '수족냉증', 3: '식중독', 4: '질염'}\n",
    "# mapping = dict(zip(range(len(encoder.classes_)), encoder.classes_))\n",
    "# mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9904651",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../korean/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aed84ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title_Article</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18424</th>\n",
       "      <td>일 만에 만잔 팔린 스벅 봄맞이 음료가 돌아온다스타벅스커피코리아 일 슈크림 라떼 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13208</th>\n",
       "      <td>산은 조원 규모 재무안정 동행프로그램 종 출시서울 신효령 산업은행은 신종 코로나바이...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>코스피 상승 마감 선 회복종합코스피 소폭 하락 출발 서울 박동주 코스피가 소폭 하락...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19740</th>\n",
       "      <td>마음여행 지구 두바퀴반 세계일주 가장 필요한건 용기와 결단도전하지 않으면 아무 일도...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11043</th>\n",
       "      <td>우리들제약 팜젠사이언스 로 사명 변경제 기 주총서 사명 변경 및 박성문 신규 감사 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18698</th>\n",
       "      <td>중국판 백신여권 한국에서도 통할까 방역당국 답변은머니 안정준 서울 뉴스 안은나 손영...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>KB운용 SK증권에서 SKIET 청약하고 KBSTAR ETF 주 받자KB자산운용이 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8197</th>\n",
       "      <td>코스피 시대 쩐의 전쟁 화수분 동학개미 VS 내다파는 기관개인투자자 하루 조 역대 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5837</th>\n",
       "      <td>아파트 단지에 영화관이 현대건설메가박스 업무협약 체결코로나 시대 맞아 변화된 트렌드...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4437</th>\n",
       "      <td>삼성엔지니어링 유가 상승으로 수주 개선 목표가 신한금융투자 신한금융투자는 일 삼성엔...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Title_Article  sentiment\n",
       "18424   일 만에 만잔 팔린 스벅 봄맞이 음료가 돌아온다스타벅스커피코리아 일 슈크림 라떼 ...          1\n",
       "13208  산은 조원 규모 재무안정 동행프로그램 종 출시서울 신효령 산업은행은 신종 코로나바이...          1\n",
       "3365   코스피 상승 마감 선 회복종합코스피 소폭 하락 출발 서울 박동주 코스피가 소폭 하락...          0\n",
       "19740  마음여행 지구 두바퀴반 세계일주 가장 필요한건 용기와 결단도전하지 않으면 아무 일도...          0\n",
       "11043  우리들제약 팜젠사이언스 로 사명 변경제 기 주총서 사명 변경 및 박성문 신규 감사 ...          0\n",
       "18698  중국판 백신여권 한국에서도 통할까 방역당국 답변은머니 안정준 서울 뉴스 안은나 손영...          0\n",
       "233    KB운용 SK증권에서 SKIET 청약하고 KBSTAR ETF 주 받자KB자산운용이 ...          0\n",
       "8197   코스피 시대 쩐의 전쟁 화수분 동학개미 VS 내다파는 기관개인투자자 하루 조 역대 ...          1\n",
       "5837   아파트 단지에 영화관이 현대건설메가박스 업무협약 체결코로나 시대 맞아 변화된 트렌드...          1\n",
       "4437   삼성엔지니어링 유가 상승으로 수주 개선 목표가 신한금융투자 신한금융투자는 일 삼성엔...          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.read_csv(PATH + \"sentiment_sample.csv\")\n",
    "new_data.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d211825",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for q, label in zip(new_data['Title_Article'], new_data['sentiment'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edaf6e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['경기 회복 기대감에 상가 거래량 증가 힐스 에비뉴 신방화역 주목최근 백신 접종률이 증가하면서 경기가 회복 될 것이라는 기대감 속에 부동산 시장도 활기를 찾는 모습이다 실제로 최근 전국 부동산 거래도 늘고 있다 실거래 자료를 보면 올해 월 전국 상업 업무용 부동산 거래량은 총 건으로 전년동월 년 월 건 대비 약 증가했다 이러한 가운데 현대건설이 서울 역세권 입지에 신규 상가를 공급해 주목 받고 있다 힐스 에비뉴 신방화역 은 서울시 강서구 방화동 일원에서 프리미엄 브랜드 힐스 에비뉴 로 공급되며 지하 층 지상 층 총 실 규모로 구성된다 지하철 호선 신방화역 바로 앞에 위치한 초역세권 대로변 상가로 조성돼 서울 강남과 여의도 김포공항 등에서 근무하는 직장인 유동인구가 풍부하다 실제로 서울시 지하철 승하차 인원 자료를 보면 지난해 월 기준 신방화역을 이용한 승하차 인원은 총 만 명으로 인근에 위치한 호선 송정역 이용객 만 명 호선 공항시장역 이용객 명을 웃돌았다 아울러 아파트 입주민을 비롯해 주변으로 풍부한 주거 수요를 갖춘다 마곡엠밸리 단지 세대 를 비롯해 마곡 힐스테이트 세대 마곡 푸르지오 세대 등 상업시설 반경 km 내에 약 만 세대가 넘는 대규모 주거타운이 조성돼 있다 여기에 방화뉴타운 초입에 위치해 총 만 여 세대의 주거 수요를 품을 전망이다 특히 대로변에 위치해 있어 도로 접근성이 우수하고 주변 단지 입주민의 유입이 수월하다 한편 견본주택은 서울시 강서구 마곡동 번지 층에 위치해 있다 ', '1']\n",
      "['갑질논란 휘말리면 어쩌나 면접관 자기검열머니 이재윤 구인구직 매칭플랫폼 사람인은 개 기업을 대상으로 면접 갑질 논란 에 대한 우려를 하고 있냐는 설문조사를 실시한 결과 가 그렇다고 답했다고 일 밝혔다 면접관들은 면접 시 자기검열을 강화하고 있다 고 답했다 이유로는 회사 관리를 위해 복수응답 라는 답변이 단연 많았다 이어 좋은 인재를 뽑기 위해서 면접 갑질이 사회적인 이슈가 되어서 문제 발생 시 불이익을 받을 수 있어서 회사의 내부 방침이어서 지원자로부터 안 좋은 피드백을 받은 적이 있어서 등의 의견이 있었다 조심하는 질문 유형으로는 성차별 소지가 있는 질문 복수응답 이 위를 차지했다 다음으로 애인 유무 등 개인사 관련 질문 부모 집안 등 배경 관련 질문 신체 조건 외모 관련 질문 출신학교 등 학력 학벌 관련 질문 정치 성향 질문 종교 관련 질문 나이 관련 질문 등의 순이었다 태도 측면에서는 지원자 자소서 숙지 등 사전 준비 복수응답 를 첫 번째로 꼽았다 면접 시 서류를 전혀 검토하지 않고 오는 등 면접관의 성의 없고 미흡한 준비가 논란이 되는 경우가 있었기 때문이다 이외에도 바른 자세로 착석 등 전반적 태도 입 퇴장 시 인사 등 기본 매너 지원자에 대한 반말 자제 면접 시간에 지각하지 않도록 함 말을 끊지 않는 등 경청 노력 지원자 질문에 대한 성심 어린 답변 을 하는 등의 노력을 하고 있었다 면접관 개인의 자기검열은 강화되는 추세지만 기업 차원의 노력은 여전히 부족했다 면접관 교육이나 면접 관련 매뉴얼을 하는 기업은 곳 중 곳 에 그쳤다 특히 기업규모별 편차가 컸다 대기업 가 면접관 교육을 받거나 매뉴얼이 있다고 응답한 반면 중소기업 은 에 그쳐 약 배 가량 차이를 보였다 ', '0']\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0])\n",
    "print(data_list[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d61d0b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea60cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: 16000\n",
      "test shape is: 4000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train / Test set 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "print(\"train shape is:\", len(dataset_train))\n",
    "print(\"test shape is:\", len(dataset_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa82f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kobert 입력 데이터로 만들기\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc8d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 20 # 훈련 반복횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0d2f315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "# 기본 Bert tokenizer 사용\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28b6dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 패딩화\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5353882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2, 4243, 7147, 4197, 6398, 6579, 1651, 1931, 1931, 2034,  517,\n",
       "        5713, 2287, 1316, 6527, 7970, 3634, 3042, 6999, 7147, 2804, 2426,\n",
       "        3463, 7408, 4919,  638,  273, 4297, 6951, 7828, 1196,  993, 5424,\n",
       "        6263, 4203, 6402, 6585, 1434, 5760, 4243, 7148,  517, 5712, 1946,\n",
       "        2287, 6079, 1557, 4243, 6416, 4197, 6398, 6579, 1426, 7147, 4257,\n",
       "        4243, 7147, 2536, 1864, 1255, 7869, 3803, 1162,    3], dtype=int32),\n",
       " array(64, dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       dtype=int32),\n",
       " 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]\n",
    "#첫 번째는 패딩된 시퀀스\n",
    "#두 번째는 길이와 타입에 대한 내용\n",
    "#세 번재는 어텐션 마스크 시퀀스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "992041c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch 형식의 dataset 만들어주기 (pytorch용 DataLoader 사용)\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "768c6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kobert 학습모델 만들기\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 2, # softmax 사용 <- binary일 경우는 2\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66a65790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install pytorch torchvision cudatoolkit=10.1 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bc1ab0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f47b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert 모델 불러오기\n",
    "\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "\n",
    "# optimizer설정 하고 schedule 설정(linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# 옵티마이저 선언\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd2045cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f043c840a20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "  \n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4dc9cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5daf5e1107845a0ab22eadc5872d156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.6924657225608826 train acc 0.515625\n",
      "epoch 1 batch id 201 loss 0.4936772286891937 train acc 0.6388370646766169\n",
      "epoch 1 train acc 0.663875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cc0459d6fc46d8a7756958eb895157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.7748015873015873\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f37f869ec574c91a1db46caee569ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.4884887933731079 train acc 0.71875\n",
      "epoch 2 batch id 201 loss 0.3973309099674225 train acc 0.779306592039801\n",
      "epoch 2 train acc 0.7875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9fed6a5bb84fd49bb3eabedeb9a56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.7958829365079365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef86487a0f9a4f1ab1700306ec5c4081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.35349172353744507 train acc 0.84375\n",
      "epoch 3 batch id 201 loss 0.19734635949134827 train acc 0.847714552238806\n",
      "epoch 3 train acc 0.854625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c8d67e5b72454a8765c68156c6a1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.7951388888888888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66dbe7d444b644aab6b0d13f26c56a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.1655658334493637 train acc 0.9375\n",
      "epoch 4 batch id 201 loss 0.09452647715806961 train acc 0.898554104477612\n",
      "epoch 4 train acc 0.9036875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c4527885404ce1add795429caa31d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.7562003968253969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603eeeca025f4682ac54f122ab4a81d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.19797645509243011 train acc 0.875\n",
      "epoch 5 batch id 201 loss 0.12477117031812668 train acc 0.933613184079602\n",
      "epoch 5 train acc 0.9345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115deeb8f48d4d0f8a6e04367636679f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.7926587301587301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901a1c9be34f4033b9885102a13d6c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.026279030367732048 train acc 1.0\n",
      "epoch 6 batch id 201 loss 0.09002722054719925 train acc 0.955146144278607\n",
      "epoch 6 train acc 0.95475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fa4952806b4960b6e5f729c82466e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.8015873015873016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fe5ce7573144eab9e6e0e2914035e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.059066519141197205 train acc 0.96875\n",
      "epoch 7 batch id 201 loss 0.012009776197373867 train acc 0.9695273631840796\n",
      "epoch 7 train acc 0.97125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515d6fa6e00f4576bd1494b2e22ecfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 test acc 0.7958829365079365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e099663f95f64924a053b3c48187d399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 0.03526568040251732 train acc 0.984375\n",
      "epoch 8 batch id 201 loss 0.010573259554803371 train acc 0.976523631840796\n",
      "epoch 8 train acc 0.9766875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f51f8ced2d446edaed8c656c19fc92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 test acc 0.796875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06345d68435747d0918aaf048e8197fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 batch id 1 loss 0.009131111204624176 train acc 1.0\n",
      "epoch 9 batch id 201 loss 0.01208426896482706 train acc 0.9821983830845771\n",
      "epoch 9 train acc 0.9830625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e9447c344a4ed5892e8355143d4360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 test acc 0.8020833333333334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b2cf5e215744b489019b9743d3daf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 batch id 1 loss 0.00625647185370326 train acc 1.0\n",
      "epoch 10 batch id 201 loss 0.0020545339211821556 train acc 0.9898942786069652\n",
      "epoch 10 train acc 0.9904375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87bffe7d3634eb58ad0a0159d0fedec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 test acc 0.7941468253968254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab6299fd505430998130722a568a66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 batch id 1 loss 0.01400135736912489 train acc 1.0\n",
      "epoch 11 batch id 201 loss 0.0018467879854142666 train acc 0.9925373134328358\n",
      "epoch 11 train acc 0.9928125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efef1a4b9f74a2ca8eed48aa13ba792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 test acc 0.794890873015873\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86544348384746d4a4994a5191d30bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 batch id 1 loss 0.0012281052768230438 train acc 1.0\n",
      "epoch 12 batch id 201 loss 0.05768979713320732 train acc 0.9948694029850746\n",
      "epoch 12 train acc 0.9949375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ad6c8525524947b01083a8317153a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 test acc 0.7996031746031746\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c95b5aa02b4709816f9f16cfa0e237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 batch id 1 loss 0.0006155276787467301 train acc 1.0\n",
      "epoch 13 batch id 201 loss 0.000673290400300175 train acc 0.9970460199004975\n",
      "epoch 13 train acc 0.9971875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df49e9e1bb14a7caa506c659b5f03f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 test acc 0.7973710317460317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fb7a5c2b994045bd2ef0d0125f6bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 batch id 1 loss 0.043217845261096954 train acc 0.96875\n",
      "epoch 14 batch id 201 loss 0.0005589465145021677 train acc 0.9977456467661692\n",
      "epoch 14 train acc 0.998\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22054712f2ab4c6b907ef91606176d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 test acc 0.8023313492063492\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752b14f5a08d4eb2bd316a84e2980786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 batch id 1 loss 0.0005285856314003468 train acc 1.0\n",
      "epoch 15 batch id 201 loss 0.00042581939487718046 train acc 0.9986007462686567\n",
      "epoch 15 train acc 0.99875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bc6589956d4a818a559cbd696b742a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 test acc 0.798859126984127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a242f7d287ca4cc69aafd1ae3e0fe76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 batch id 1 loss 0.0004874920705333352 train acc 1.0\n",
      "epoch 16 batch id 201 loss 0.0004398644086904824 train acc 0.9990671641791045\n",
      "epoch 16 train acc 0.999125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a899126e6b4a9baf08636d800b1ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 test acc 0.798859126984127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3613481cbef847a092e211611a91523d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 batch id 1 loss 0.00047067267587408423 train acc 1.0\n",
      "epoch 17 batch id 201 loss 0.0003748704039026052 train acc 0.9992226368159204\n",
      "epoch 17 train acc 0.9991875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280b3bf1e10948bb87ce8ce81867a66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 test acc 0.7981150793650794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee39eab178f48b6bd7e532f4db21aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 batch id 1 loss 0.0003964520001318306 train acc 1.0\n",
      "epoch 18 batch id 201 loss 0.0003655586624518037 train acc 0.9993781094527363\n",
      "epoch 18 train acc 0.999375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cba7c19dda48329c67626068ccea54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 test acc 0.7986111111111112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed65dd8484db479e8e2665d6cab9b605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 batch id 1 loss 0.00035120698157697916 train acc 1.0\n",
      "epoch 19 batch id 201 loss 0.00031382651650346816 train acc 0.9996113184079602\n",
      "epoch 19 train acc 0.9995625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c731c148feb426794d62fcbd17726a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 test acc 0.7986111111111112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d0786e9a2245348983139ad93acedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 batch id 1 loss 0.0006921594613231719 train acc 1.0\n",
      "epoch 20 batch id 201 loss 0.00035671930527314544 train acc 0.9996113184079602\n",
      "epoch 20 train acc 0.999625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f9d65afdca418bae5fc7aec8cebbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 test acc 0.798859126984127\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 시작\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5d10466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32173da9f563445e9def2080602208fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea823b1c0c042778e10611dc3cef883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 test acc 3.195436507936508\n"
     ]
    }
   ],
   "source": [
    "# 테스트 문장 예측\n",
    "test_sentence = \"본격적으로 막이 오른 쌍용차 인수전이 SM그룹의 '깜짝 등판'으로 후끈 달아오른 가운데 당사자인 쌍용차 역시 예상외의 흥행에 미소 지으며 후속 작업을 준비하고 있다. 최근 쌍용차가 친환경차 전용공장 건설 계획을 밝히는 등 전기차 전환에 뒤늦게 속도를 내는 가운데 상당수의 투자자가 전기차 사업 확대를 목표로 인수 의향을 밝히고 나선 것도 긍정적이라는 분위기다.\"\n",
    "test_label = 1 # 실제 값\n",
    "\n",
    "unseen_test = pd.DataFrame([[test_sentence, test_label]], columns = [['title', 'cls']])\n",
    "unseen_values = unseen_test.values\n",
    "test_set = BERTDataset(unseen_values, 0, 1, tok, max_len, True, False)\n",
    "test_input = torch.utils.data.DataLoader(test_set, batch_size=1, num_workers=5)\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_input)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    print(out)\n",
    "    \n",
    "model.eval() # 평가 모드로 변경\n",
    "    \n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f027f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성값 예측하는 함수 만들기\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 위에서 설정한 tok, max_len, batch_size, device를 그대로 입력\n",
    "# comment : 예측하고자 하는 텍스트 데이터 리스트\n",
    "def getSentimentValue(comment, tok, max_len, batch_size, device):\n",
    "    commnetslist = [] # 텍스트 데이터를 담을 리스트\n",
    "    emo_list = [] # 감성 값을 담을 리스트\n",
    "    for c in comment: # 모든 댓글\n",
    "        commnetslist.append( [c, 5] ) # [댓글, 임의의 양의 정수값] 설정\n",
    "\n",
    "    pdData = pd.DataFrame( commnetslist, columns = [['뉴스', '감성']] )\n",
    "    pdData = pdData.values\n",
    "    test_set = BERTDataset(pdData, 0, 1, tok, max_len, True, False) \n",
    "    test_input = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_input):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length \n",
    "        # 이때, out이 예측 결과 리스트\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        print(out)\n",
    "        # e는 2가지 실수 값으로 구성된 리스트\n",
    "        # 0번 인덱스가 더 크면 부정, 긍정은 반대\n",
    "        for e in out:\n",
    "            if e[0]>e[1]: # 부정\n",
    "                value = 0\n",
    "                emo_list.append(\"부정\")\n",
    "            else: #긍정\n",
    "                value = 1\n",
    "                emo_list.append(\"긍정\")\n",
    "\n",
    "    return emo_list # 텍스트 데이터에 1대1 매칭되는 감성값 리스트 반환\n",
    "\n",
    "# input : 텍스트 데이터 리스트 외 KoBERT 설정 파라미터들\n",
    "# output : 입력한 텍스트 데이터 리스트와 1대 1 매칭 되는 감성 값 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf2dcd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def news():\n",
    "    \n",
    "    comment = []\n",
    "    comment.append(input(\"원하는 기사를 입력하세요\"))\n",
    "\n",
    "    for c in comment:\n",
    "        print(f'\\n기사 : {c}\\n')\n",
    "        \n",
    "    getSentimentValue(comment, tok, max_len, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35ed7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원하는 기사를 입력하세요삼성전자와 SK하이닉스 등 대형 반도체주 강세에 코스피가 3,230선을 회복했습니다.  코스피지수는 전 거래일보다 0.44% 오른 3,237.14에 거래를 마쳤습니다.  삼성전자는 외국인이 6,000억 원 넘게 사들이면서 13거래일 만에 8만 원 선을 회복했고, SK하이닉스도 3% 넘게 올랐습니다.  다만 중국에서 규제 리스크가 불거진 게임주가 약세를 보이며 상승폭을 제한했습니다.\n",
      "\n",
      "기사 : 삼성전자와 SK하이닉스 등 대형 반도체주 강세에 코스피가 3,230선을 회복했습니다.  코스피지수는 전 거래일보다 0.44% 오른 3,237.14에 거래를 마쳤습니다.  삼성전자는 외국인이 6,000억 원 넘게 사들이면서 13거래일 만에 8만 원 선을 회복했고, SK하이닉스도 3% 넘게 올랐습니다.  다만 중국에서 규제 리스크가 불거진 게임주가 약세를 보이며 상승폭을 제한했습니다.\n",
      "\n",
      "tensor([[-3.7498,  4.4319]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5870577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py36_pytorch",
   "language": "python",
   "name": "conda-env-azureml_py36_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
