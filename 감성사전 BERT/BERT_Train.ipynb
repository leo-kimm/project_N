{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0de44c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SKT Brain github 주소는 다음과 같습니다. https://github.com/SKTBrain/KoBERT\n",
    "\n",
    "# !pip install mxnet\n",
    "# !pip install gluonnlp pandas tqdm\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers==3 # 최신 버전으로 설치하면 \"Input: must be Tensor, not str\" 라는 에러 발생\n",
    "# !pip install torch\n",
    "\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# kobert \n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "# transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96dc11e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "Available devices  1\n",
      "Current cuda device  0\n",
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 시\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Bert모델, Voca 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "# cuda device 확인\n",
    "torch.cuda. empty_cache()\n",
    "\n",
    "#GPU 디바이스의 갯수\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "#현재 셋업 되어있는 GPU 넘버\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "#GPU 디바이스의 이름\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9904651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path 설정\n",
    "PATH = '../korean/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aed84ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title_Article</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218986</th>\n",
       "      <td>기준치 초과 어린이 가죽 지갑 제품 리콜납 기준 초과 어린이 가죽 지갑 안전 기준 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340360</th>\n",
       "      <td>롯데건설 층간 소음 기술 개발 공동 연구 협약 추진 경량 중량 충격음 모두 완충재 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164392</th>\n",
       "      <td>올댓차이 중국 증시 혼조 마감 서울 문예 중국 증시 혼조 마감 이날 상하 종합 지수...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256774</th>\n",
       "      <td>화두 증세 인플레 빅테크 실적 주목 월가 시각머니 뉴욕 임동욱 특파원 사람 법인세 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89345</th>\n",
       "      <td>보드 업계 법규제 완화 이상 서비스내달 이용가 법개정 주요 업체 자발 연령 제한 최...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88295</th>\n",
       "      <td>한전 올해 공정 거래 자율 준수 프로그램 운영한국전력 지속 자율 공정 거래 준수 체...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223347</th>\n",
       "      <td>육박 추월 나랏빚 회계 연도 국가 결산 국민 만원 나라살림 관리 재정 수지 적자 역...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66230</th>\n",
       "      <td>리테일 공정위 장기 점포 상생 협약 가맹점 파트너십 강화 김아 리테일 서울시 중구 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340838</th>\n",
       "      <td>현장 연결 부동산 투기 예방 적발 일벌 환수 대책 마련 정부 조금 홍남기 경제 부총...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12386</th>\n",
       "      <td>수출 이제 디지털 대전환 코트라 빅데이터 이코리아 간담회코 지나 서울 삼성동 디지털...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Title_Article  sentiment\n",
       "218986  기준치 초과 어린이 가죽 지갑 제품 리콜납 기준 초과 어린이 가죽 지갑 안전 기준 ...          0\n",
       "340360  롯데건설 층간 소음 기술 개발 공동 연구 협약 추진 경량 중량 충격음 모두 완충재 ...          1\n",
       "164392  올댓차이 중국 증시 혼조 마감 서울 문예 중국 증시 혼조 마감 이날 상하 종합 지수...          0\n",
       "256774  화두 증세 인플레 빅테크 실적 주목 월가 시각머니 뉴욕 임동욱 특파원 사람 법인세 ...          1\n",
       "89345   보드 업계 법규제 완화 이상 서비스내달 이용가 법개정 주요 업체 자발 연령 제한 최...          0\n",
       "88295   한전 올해 공정 거래 자율 준수 프로그램 운영한국전력 지속 자율 공정 거래 준수 체...          1\n",
       "223347  육박 추월 나랏빚 회계 연도 국가 결산 국민 만원 나라살림 관리 재정 수지 적자 역...          0\n",
       "66230   리테일 공정위 장기 점포 상생 협약 가맹점 파트너십 강화 김아 리테일 서울시 중구 ...          1\n",
       "340838  현장 연결 부동산 투기 예방 적발 일벌 환수 대책 마련 정부 조금 홍남기 경제 부총...          0\n",
       "12386   수출 이제 디지털 대전환 코트라 빅데이터 이코리아 간담회코 지나 서울 삼성동 디지털...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv 파일 받아와서 샘플 10개 뽑아보기\n",
    "new_data = pd.read_csv(PATH + 'sdata11.csv')\n",
    "new_data.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d211825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data list 만들어 sentiment 숫자값에서 str으로 변환시켜주기(모델 들어가기위해)\n",
    "\n",
    "data_list = []\n",
    "for q, label in zip(new_data['Title_Article'], new_data['sentiment'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)\n",
    "    \n",
    "# 만든 data list 형변환 확인\n",
    "type(data_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea60cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: 311592\n",
      "test shape is: 77899\n",
      "total shape is: 389491\n"
     ]
    }
   ],
   "source": [
    "# Train / Test set 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "\n",
    "# data 사이즈 확인\n",
    "print(\"train shape is:\", len(dataset_train))\n",
    "print(\"test shape is:\", len(dataset_test))\n",
    "print(\"total shape is:\", len(dataset_train)+len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa82f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kobert 입력 데이터로 만들기\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc8d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters 파라미터 세팅\n",
    "max_len = 64 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 9 # 훈련 반복횟수\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0d2f315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "# 기본 Bert tokenizer 사용\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b6dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 패딩화\n",
    "\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5353882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2,  517, 6147, 3758, 6510, 7489, 7947, 4152, 4768, 2785, 5678,\n",
       "        2155, 4768,  517, 6591, 6553, 7209, 6879,  517, 6591, 7533, 4768,\n",
       "        3533, 5550, 1633, 6280, 2650, 6951, 6896, 6664, 6837, 1370, 5725,\n",
       "        6432, 2169, 7619, 1267, 4768, 6874, 3468, 6198, 4152, 2337, 2734,\n",
       "        7234, 2238, 2822, 6406, 1132, 6516, 1211, 5753, 5954, 2195, 2442,\n",
       "        1283, 3902, 2991,  517, 7922, 6812,  517, 6147,    3], dtype=int32),\n",
       " array(64, dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       dtype=int32),\n",
       " 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 패딩화 잘 되었는지 확인\n",
    "\n",
    "data_train[0]\n",
    "\n",
    "#첫 번째는 패딩된 시퀀스\n",
    "#두 번째는 길이와 타입에 대한 내용\n",
    "#세 번재는 어텐션 마스크 시퀀스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "992041c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch 형식의 dataset 만들어주기 (pytorch용 DataLoader 사용)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "768c6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kobert 학습모델 만들기\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 2, # softmax 사용 <- binary일 경우는 2\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f47b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert 모델 불러오기\n",
    "\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "\n",
    "# optimizer설정 하고 schedule 설정(linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# 옵티마이저 선언\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n",
    "#loss_fn = nn.BCEWithLogitsLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd2045cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fe664740898>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "  \n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4dc9cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34dc7c60030487cbccbb2b90250c454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7361290454864502 train acc 0.484375\n",
      "epoch 1 batch id 201 loss 0.6084112524986267 train acc 0.5689521144278606\n",
      "epoch 1 batch id 401 loss 0.4131428897380829 train acc 0.6837593516209476\n",
      "epoch 1 batch id 601 loss 0.4295541048049927 train acc 0.7376767886855241\n",
      "epoch 1 batch id 801 loss 0.2976975739002228 train acc 0.7680633583021224\n",
      "epoch 1 batch id 1001 loss 0.41340136528015137 train acc 0.7902878371628371\n",
      "epoch 1 batch id 1201 loss 0.2814720869064331 train acc 0.80564373438801\n",
      "epoch 1 batch id 1401 loss 0.30909910798072815 train acc 0.817396056388294\n",
      "epoch 1 batch id 1601 loss 0.16753767430782318 train acc 0.8270612117426608\n",
      "epoch 1 batch id 1801 loss 0.16925112903118134 train acc 0.8352477790116601\n",
      "epoch 1 batch id 2001 loss 0.14112988114356995 train acc 0.8417275737131434\n",
      "epoch 1 batch id 2201 loss 0.3019237816333771 train acc 0.847320820081781\n",
      "epoch 1 batch id 2401 loss 0.21028688549995422 train acc 0.8514746459808413\n",
      "epoch 1 batch id 2601 loss 0.10534945130348206 train acc 0.8554702518262207\n",
      "epoch 1 batch id 2801 loss 0.3068169951438904 train acc 0.8592690110674759\n",
      "epoch 1 batch id 3001 loss 0.18357771635055542 train acc 0.8623948267244252\n",
      "epoch 1 batch id 3201 loss 0.14455746114253998 train acc 0.8649640737269603\n",
      "epoch 1 batch id 3401 loss 0.1890552043914795 train acc 0.8671162893266686\n",
      "epoch 1 batch id 3601 loss 0.2522970736026764 train acc 0.8691465912246599\n",
      "epoch 1 batch id 3801 loss 0.13955442607402802 train acc 0.8711523283346487\n",
      "epoch 1 batch id 4001 loss 0.24373546242713928 train acc 0.8728286678330417\n",
      "epoch 1 batch id 4201 loss 0.18274709582328796 train acc 0.8744420971197334\n",
      "epoch 1 batch id 4401 loss 0.25117647647857666 train acc 0.8759408373097023\n",
      "epoch 1 batch id 4601 loss 0.3084542453289032 train acc 0.8774587046294284\n",
      "epoch 1 batch id 4801 loss 0.2641758918762207 train acc 0.8788859091855863\n",
      "epoch 1 train acc 0.8793309714520434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2545ce4d34db4d128c9c92bd82d7c619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.9197349884311092\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73714ae3e9a7440f8e925bb9fa188f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.3768395185470581 train acc 0.84375\n",
      "epoch 2 batch id 201 loss 0.33471664786338806 train acc 0.9137904228855721\n",
      "epoch 2 batch id 401 loss 0.3154512047767639 train acc 0.9121726932668329\n",
      "epoch 2 batch id 601 loss 0.2415381520986557 train acc 0.913321547420965\n",
      "epoch 2 batch id 801 loss 0.25615227222442627 train acc 0.9137211298377028\n",
      "epoch 2 batch id 1001 loss 0.19419129192829132 train acc 0.9149912587412588\n",
      "epoch 2 batch id 1201 loss 0.22168266773223877 train acc 0.9155651540383014\n",
      "epoch 2 batch id 1401 loss 0.18514472246170044 train acc 0.9157521413276232\n",
      "epoch 2 batch id 1601 loss 0.10047618299722672 train acc 0.9160680824484697\n",
      "epoch 2 batch id 1801 loss 0.1520732194185257 train acc 0.9166435313714603\n",
      "epoch 2 batch id 2001 loss 0.11278922855854034 train acc 0.9171273738130935\n",
      "epoch 2 batch id 2201 loss 0.17684194445610046 train acc 0.9179279304861426\n",
      "epoch 2 batch id 2401 loss 0.2270849496126175 train acc 0.9183413161182841\n",
      "epoch 2 batch id 2601 loss 0.08718056231737137 train acc 0.9189073913879278\n",
      "epoch 2 batch id 2801 loss 0.2796650528907776 train acc 0.9197105944305605\n",
      "epoch 2 batch id 3001 loss 0.14385423064231873 train acc 0.9203390536487838\n",
      "epoch 2 batch id 3201 loss 0.13012221455574036 train acc 0.9205863402061856\n",
      "epoch 2 batch id 3401 loss 0.17346471548080444 train acc 0.9211628932666863\n",
      "epoch 2 batch id 3601 loss 0.12286746501922607 train acc 0.9217708622604832\n",
      "epoch 2 batch id 3801 loss 0.1183662861585617 train acc 0.922216193107077\n",
      "epoch 2 batch id 4001 loss 0.11823073029518127 train acc 0.9227029180204949\n",
      "epoch 2 batch id 4201 loss 0.08273142576217651 train acc 0.9234259700071411\n",
      "epoch 2 batch id 4401 loss 0.16308072209358215 train acc 0.9240584526244036\n",
      "epoch 2 batch id 4601 loss 0.16811007261276245 train acc 0.924754808737231\n",
      "epoch 2 batch id 4801 loss 0.20059724152088165 train acc 0.9253866382003749\n",
      "epoch 2 train acc 0.925564797699733\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4d4ada84d54ecd8b86eeeead998ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.9269188964770863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8115b0e315bf4cc5bd48b331fa54aeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.32480111718177795 train acc 0.90625\n",
      "epoch 3 batch id 201 loss 0.22982077300548553 train acc 0.9402207711442786\n",
      "epoch 3 batch id 401 loss 0.2583850920200348 train acc 0.9399158354114713\n",
      "epoch 3 batch id 601 loss 0.18454599380493164 train acc 0.9410357737104825\n",
      "epoch 3 batch id 801 loss 0.16833855211734772 train acc 0.9416549625468165\n",
      "epoch 3 batch id 1001 loss 0.11668340116739273 train acc 0.9422296453546454\n",
      "epoch 3 batch id 1201 loss 0.09007201343774796 train acc 0.9429121565362198\n",
      "epoch 3 batch id 1401 loss 0.1510545015335083 train acc 0.9432994289793005\n",
      "epoch 3 batch id 1601 loss 0.11801674216985703 train acc 0.9435899437851343\n",
      "epoch 3 batch id 1801 loss 0.15027806162834167 train acc 0.9440762076624097\n",
      "epoch 3 batch id 2001 loss 0.06770829856395721 train acc 0.9446995252373813\n",
      "epoch 3 batch id 2201 loss 0.10404449701309204 train acc 0.945280554293503\n",
      "epoch 3 batch id 2401 loss 0.08755512535572052 train acc 0.9457517700957934\n",
      "epoch 3 batch id 2601 loss 0.04775518923997879 train acc 0.9464989427143407\n",
      "epoch 3 batch id 2801 loss 0.2252892255783081 train acc 0.9470668957515174\n",
      "epoch 3 batch id 3001 loss 0.07201854884624481 train acc 0.9475799733422192\n",
      "epoch 3 batch id 3201 loss 0.06416365504264832 train acc 0.9478776163698844\n",
      "epoch 3 batch id 3401 loss 0.11726126074790955 train acc 0.9484802264039989\n",
      "epoch 3 batch id 3601 loss 0.09263686835765839 train acc 0.9491026798111636\n",
      "epoch 3 batch id 3801 loss 0.06172682344913483 train acc 0.949532195474875\n",
      "epoch 3 batch id 4001 loss 0.07983620464801788 train acc 0.9498133279180205\n",
      "epoch 3 batch id 4201 loss 0.08752624690532684 train acc 0.9502090276124732\n",
      "epoch 3 batch id 4401 loss 0.10552603751420975 train acc 0.9506965746421268\n",
      "epoch 3 batch id 4601 loss 0.18736115097999573 train acc 0.9512538035209737\n",
      "epoch 3 batch id 4801 loss 0.12757691740989685 train acc 0.9517483336804833\n",
      "epoch 3 train acc 0.9518972068186486\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec3314d71ba4f28927bf95ff6e34531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.9262389908941634\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504cfbc53f304b899ac5963bf805a52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.18207429349422455 train acc 0.953125\n",
      "epoch 4 batch id 201 loss 0.1955815702676773 train acc 0.9598880597014925\n",
      "epoch 4 batch id 401 loss 0.15775258839130402 train acc 0.9610738778054863\n",
      "epoch 4 batch id 601 loss 0.02484392374753952 train acc 0.9611584858569051\n",
      "epoch 4 batch id 801 loss 0.15698514878749847 train acc 0.9620396379525593\n",
      "epoch 4 batch id 1001 loss 0.022893249988555908 train acc 0.9629120879120879\n",
      "epoch 4 batch id 1201 loss 0.016248833388090134 train acc 0.9637021232306411\n",
      "epoch 4 batch id 1401 loss 0.09859801083803177 train acc 0.964143915060671\n",
      "epoch 4 batch id 1601 loss 0.024733737111091614 train acc 0.9644167707682698\n",
      "epoch 4 batch id 1801 loss 0.05640704184770584 train acc 0.9650801637978901\n",
      "epoch 4 batch id 2001 loss 0.04003564268350601 train acc 0.965392303848076\n",
      "epoch 4 batch id 2201 loss 0.060480229556560516 train acc 0.9655980236256247\n",
      "epoch 4 batch id 2401 loss 0.0348355658352375 train acc 0.9658085172844648\n",
      "epoch 4 batch id 2601 loss 0.013722251169383526 train acc 0.9660166762783545\n",
      "epoch 4 batch id 2801 loss 0.18664862215518951 train acc 0.9664461353088183\n",
      "epoch 4 batch id 3001 loss 0.06198873370885849 train acc 0.9665371959346885\n",
      "epoch 4 batch id 3201 loss 0.016887420788407326 train acc 0.9668609418931584\n",
      "epoch 4 batch id 3401 loss 0.05416879057884216 train acc 0.9671741767127315\n",
      "epoch 4 batch id 3601 loss 0.01525645237416029 train acc 0.9675914676478756\n",
      "epoch 4 batch id 3801 loss 0.008082708343863487 train acc 0.9678949618521442\n",
      "epoch 4 batch id 4001 loss 0.015216241590678692 train acc 0.968211072231942\n",
      "epoch 4 batch id 4201 loss 0.022189101204276085 train acc 0.9685975065460605\n",
      "epoch 4 batch id 4401 loss 0.1026378944516182 train acc 0.9689026641672347\n",
      "epoch 4 batch id 4601 loss 0.12932920455932617 train acc 0.9691507281025864\n",
      "epoch 4 batch id 4801 loss 0.13544252514839172 train acc 0.9694627421370547\n",
      "epoch 4 train acc 0.9695875693160814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0521f8753784550a9191ec481372d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.9320630877742947\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af33f332d644c07a65528e40446b636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.11292776465415955 train acc 0.984375\n",
      "epoch 5 batch id 201 loss 0.032543398439884186 train acc 0.9770677860696517\n",
      "epoch 5 batch id 401 loss 0.10513468831777573 train acc 0.9757637157107232\n",
      "epoch 5 batch id 601 loss 0.009048526175320148 train acc 0.9762635191347754\n",
      "epoch 5 batch id 801 loss 0.12533579766750336 train acc 0.9769233770287141\n",
      "epoch 5 batch id 1001 loss 0.038896266371011734 train acc 0.977022977022977\n",
      "epoch 5 batch id 1201 loss 0.059986911714076996 train acc 0.9774146544546212\n",
      "epoch 5 batch id 1401 loss 0.05404357239603996 train acc 0.9776498929336188\n",
      "epoch 5 batch id 1601 loss 0.009149052202701569 train acc 0.9779141942535915\n",
      "epoch 5 batch id 1801 loss 0.03483099490404129 train acc 0.9782239033870073\n",
      "epoch 5 batch id 2001 loss 0.07167820632457733 train acc 0.978604447776112\n",
      "epoch 5 batch id 2201 loss 0.01011965423822403 train acc 0.978851942298955\n",
      "epoch 5 batch id 2401 loss 0.03113763965666294 train acc 0.9790647126197418\n",
      "epoch 5 batch id 2601 loss 0.05069969594478607 train acc 0.9791005863129566\n",
      "epoch 5 batch id 2801 loss 0.1323777735233307 train acc 0.9793990985362371\n",
      "epoch 5 batch id 3001 loss 0.004598445724695921 train acc 0.9795641036321227\n",
      "epoch 5 batch id 3201 loss 0.01515008881688118 train acc 0.9796499140893471\n",
      "epoch 5 batch id 3401 loss 0.007953123189508915 train acc 0.9798404880917377\n",
      "epoch 5 batch id 3601 loss 0.007214312441647053 train acc 0.9799534851430158\n",
      "epoch 5 batch id 3801 loss 0.013108327984809875 train acc 0.9800422586161537\n",
      "epoch 5 batch id 4001 loss 0.012384315021336079 train acc 0.9801963571607099\n",
      "epoch 5 batch id 4201 loss 0.098774254322052 train acc 0.9803729766722209\n",
      "epoch 5 batch id 4401 loss 0.14093297719955444 train acc 0.9806294024085435\n",
      "epoch 5 batch id 4601 loss 0.12364046275615692 train acc 0.9808567430993262\n",
      "epoch 5 batch id 4801 loss 0.07401356846094131 train acc 0.9810391064361591\n",
      "epoch 5 train acc 0.9810889042924625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06324389c8d74dcdb2f36c423224776b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.9328712774294672\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03118ac44be240cdb296ee74dbacbcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.1611035019159317 train acc 0.953125\n",
      "epoch 6 batch id 201 loss 0.004268002696335316 train acc 0.9847636815920398\n",
      "epoch 6 batch id 401 loss 0.033363405615091324 train acc 0.9849205112219451\n",
      "epoch 6 batch id 601 loss 0.05429176986217499 train acc 0.985648918469218\n",
      "epoch 6 batch id 801 loss 0.007900716736912727 train acc 0.9857599875156055\n",
      "epoch 6 batch id 1001 loss 0.00561200175434351 train acc 0.9857174075924076\n",
      "epoch 6 batch id 1201 loss 0.010710650123655796 train acc 0.9856890091590341\n",
      "epoch 6 batch id 1401 loss 0.022207247093319893 train acc 0.9860256067094932\n",
      "epoch 6 batch id 1601 loss 0.002719887299463153 train acc 0.9864147407870081\n",
      "epoch 6 batch id 1801 loss 0.0923747569322586 train acc 0.9866480427540255\n",
      "epoch 6 batch id 2001 loss 0.12859444320201874 train acc 0.9867410044977512\n",
      "epoch 6 batch id 2201 loss 0.0016298091504722834 train acc 0.9869022603362109\n",
      "epoch 6 batch id 2401 loss 0.06406717747449875 train acc 0.9870691899208663\n",
      "epoch 6 batch id 2601 loss 0.0019620684906840324 train acc 0.9872705209534794\n",
      "epoch 6 batch id 2801 loss 0.15770405530929565 train acc 0.987482149232417\n",
      "epoch 6 batch id 3001 loss 0.0025416442658752203 train acc 0.9875718510496501\n",
      "epoch 6 batch id 3201 loss 0.0019041692139580846 train acc 0.9877430880974696\n",
      "epoch 6 batch id 3401 loss 0.0030206837691366673 train acc 0.9878206777418407\n",
      "epoch 6 batch id 3601 loss 0.0012633393052965403 train acc 0.9878853096362121\n",
      "epoch 6 batch id 3801 loss 0.014793943613767624 train acc 0.9880459089713234\n",
      "epoch 6 batch id 4001 loss 0.004419535864144564 train acc 0.9881006310922269\n",
      "epoch 6 batch id 4201 loss 0.021698063239455223 train acc 0.9882580040466555\n",
      "epoch 6 batch id 4401 loss 0.103977732360363 train acc 0.9884223755964554\n",
      "epoch 6 batch id 4601 loss 0.07976851612329483 train acc 0.9885588730710715\n",
      "epoch 6 batch id 4801 loss 0.003155328566208482 train acc 0.9886579618829411\n",
      "epoch 6 train acc 0.9886989114807969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691079a62e014231a69ed26aa698211a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.9370405097775788\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1522f91098a402d82b548dcde51c38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.16176316142082214 train acc 0.96875\n",
      "epoch 7 batch id 201 loss 0.041600652039051056 train acc 0.9908271144278606\n",
      "epoch 7 batch id 401 loss 0.05550410971045494 train acc 0.9909600997506235\n",
      "epoch 7 batch id 601 loss 0.03635244071483612 train acc 0.9911605657237936\n",
      "epoch 7 batch id 801 loss 0.028923071920871735 train acc 0.9913779650436954\n",
      "epoch 7 batch id 1001 loss 0.0026145349256694317 train acc 0.9914928821178821\n",
      "epoch 7 batch id 1201 loss 0.004382371436804533 train acc 0.991790695253955\n",
      "epoch 7 batch id 1401 loss 0.0021361028775572777 train acc 0.9919811741613134\n",
      "epoch 7 batch id 1601 loss 0.0018516301643103361 train acc 0.9921143035602749\n",
      "epoch 7 batch id 1801 loss 0.0013931415742263198 train acc 0.9924000555247084\n",
      "epoch 7 batch id 2001 loss 0.0016633388586342335 train acc 0.9924100449775113\n",
      "epoch 7 batch id 2201 loss 0.0013968677958473563 train acc 0.9924253180372558\n",
      "epoch 7 batch id 2401 loss 0.0003874879621434957 train acc 0.9925486776343191\n",
      "epoch 7 batch id 2601 loss 0.002382223028689623 train acc 0.9925929930795848\n",
      "epoch 7 batch id 2801 loss 0.08238720148801804 train acc 0.9927871742234916\n",
      "epoch 7 batch id 3001 loss 0.001073662075214088 train acc 0.9929294401866045\n",
      "epoch 7 batch id 3201 loss 0.03998766839504242 train acc 0.9930197594501718\n",
      "epoch 7 batch id 3401 loss 0.0021935952827334404 train acc 0.9930213540135254\n",
      "epoch 7 batch id 3601 loss 0.0007794343982823193 train acc 0.9930791793946127\n",
      "epoch 7 batch id 3801 loss 0.0007669197511859238 train acc 0.9931021441725861\n",
      "epoch 7 batch id 4001 loss 0.03683323413133621 train acc 0.9931696763309172\n",
      "epoch 7 batch id 4201 loss 0.0008743135840632021 train acc 0.9932642525589146\n",
      "epoch 7 batch id 4401 loss 0.003281795419752598 train acc 0.9933537832310838\n",
      "epoch 7 batch id 4601 loss 0.0007099705981090665 train acc 0.9934525103238426\n",
      "epoch 7 batch id 4801 loss 0.0009947314392775297 train acc 0.9935234846906894\n",
      "epoch 7 train acc 0.9935497535428219\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e97f22de18c42238be626bbaac4e562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 test acc 0.9378230426183013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2c4d54236b46d298cf15fd6e53b6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 0.15451738238334656 train acc 0.96875\n",
      "epoch 8 batch id 201 loss 0.001598687726072967 train acc 0.9951803482587065\n",
      "epoch 8 batch id 401 loss 0.011998903006315231 train acc 0.9951683291770573\n",
      "epoch 8 batch id 601 loss 0.001303145312704146 train acc 0.9953982945091514\n",
      "epoch 8 batch id 801 loss 0.0008703926578164101 train acc 0.9956694756554307\n",
      "epoch 8 batch id 1001 loss 0.0006033435929566622 train acc 0.9957854645354646\n",
      "epoch 8 batch id 1201 loss 0.0004989694571122527 train acc 0.9960579725228976\n",
      "epoch 8 batch id 1401 loss 0.00044005116797052324 train acc 0.9959403997144897\n",
      "epoch 8 batch id 1601 loss 0.00145623367279768 train acc 0.9959888351030606\n",
      "epoch 8 batch id 1801 loss 0.0006515547283925116 train acc 0.9960351887840089\n",
      "epoch 8 batch id 2001 loss 0.0006973931449465454 train acc 0.9960957021489255\n",
      "epoch 8 batch id 2201 loss 0.04360019415616989 train acc 0.9961665152203544\n",
      "epoch 8 batch id 2401 loss 0.0003713849000632763 train acc 0.996225531028738\n",
      "epoch 8 batch id 2601 loss 0.0006350006442517042 train acc 0.9962574490580546\n",
      "epoch 8 batch id 2801 loss 0.005721478722989559 train acc 0.9962848089967868\n",
      "epoch 8 batch id 3001 loss 0.0005967376055195928 train acc 0.9963657947350884\n",
      "epoch 8 batch id 3201 loss 0.0005317486939020455 train acc 0.9964122539831303\n",
      "epoch 8 batch id 3401 loss 0.001296080881729722 train acc 0.9964302778594532\n",
      "epoch 8 batch id 3601 loss 0.0009098317823372781 train acc 0.9964246042765899\n",
      "epoch 8 batch id 3801 loss 0.0007396309520117939 train acc 0.996460635359116\n",
      "epoch 8 batch id 4001 loss 0.019275007769465446 train acc 0.9964774431392152\n",
      "epoch 8 batch id 4201 loss 0.000619558384642005 train acc 0.9965186860271364\n",
      "epoch 8 batch id 4401 loss 0.0011072958586737514 train acc 0.996559730743013\n",
      "epoch 8 batch id 4601 loss 0.001986112678423524 train acc 0.9966277711367094\n",
      "epoch 8 batch id 4801 loss 0.0004667679313570261 train acc 0.9966836336179963\n",
      "epoch 8 train acc 0.9966978589032656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bb32aca6364c858f038b2b925327f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 test acc 0.9405939785788925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e8b11152a140269afdd0eaf00389f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 batch id 1 loss 0.1834559589624405 train acc 0.96875\n",
      "epoch 9 batch id 201 loss 0.0005235691787675023 train acc 0.9971237562189055\n",
      "epoch 9 batch id 401 loss 0.10129231959581375 train acc 0.99696072319202\n",
      "epoch 9 batch id 601 loss 0.0006098419544287026 train acc 0.9972181780366056\n",
      "epoch 9 batch id 801 loss 0.0005149913486093283 train acc 0.997269038701623\n",
      "epoch 9 batch id 1001 loss 0.0008067622547969222 train acc 0.9973464035964036\n",
      "epoch 9 batch id 1201 loss 0.0018036914989352226 train acc 0.997463051623647\n",
      "epoch 9 batch id 1401 loss 0.000481476541608572 train acc 0.9974906316916489\n",
      "epoch 9 batch id 1601 loss 0.00034411842352710664 train acc 0.9976284353529045\n",
      "epoch 9 batch id 1801 loss 0.00046695146011188626 train acc 0.9977269572459745\n",
      "epoch 9 batch id 2001 loss 0.00048422414693050086 train acc 0.9977901674162919\n",
      "epoch 9 batch id 2201 loss 0.00036519451532512903 train acc 0.9977850976828714\n",
      "epoch 9 batch id 2401 loss 0.00035326540819369256 train acc 0.99777436484798\n",
      "epoch 9 batch id 2601 loss 0.0009799632243812084 train acc 0.9977833044982699\n",
      "epoch 9 batch id 2801 loss 0.000637626857496798 train acc 0.9978244377008212\n",
      "epoch 9 batch id 3001 loss 0.0003800195408985019 train acc 0.9978080223258914\n",
      "epoch 9 batch id 3201 loss 0.0005019751843065023 train acc 0.9978375898156826\n",
      "epoch 9 batch id 3401 loss 0.0006120282923802733 train acc 0.9978544913260806\n",
      "epoch 9 batch id 3601 loss 0.00038049824070185423 train acc 0.9978564981949458\n",
      "epoch 9 batch id 3801 loss 0.00044612152851186693 train acc 0.9978870691923178\n",
      "epoch 9 batch id 4001 loss 0.0007707112235948443 train acc 0.9979145838540365\n",
      "epoch 9 batch id 4201 loss 0.00035247154301032424 train acc 0.9979469174006189\n",
      "epoch 9 batch id 4401 loss 0.04500025883316994 train acc 0.9979656612133606\n",
      "epoch 9 batch id 4601 loss 0.00043889303924515843 train acc 0.9979895674853293\n",
      "epoch 9 batch id 4801 loss 0.0005614630063064396 train acc 0.9979919548010832\n",
      "epoch 9 train acc 0.9980135808174163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c82533405e4eb28aeb189ceb6ae8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 test acc 0.9406068069861174\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 시작\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length \n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbd03762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장하기\n",
    "torch.save(model, PATH + 'model11.pt')  # 전체 모델 저장\n",
    "torch.save(model.state_dict(), PATH + 'model_state_dict11.pt')  # 모델 객체의 state_dict 저장\n",
    "# torch.save({\n",
    "#     'model': model.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "#     'loss_fn' : loss_fn.state_dict()\n",
    "#     't_total' : t_total.state_dict()\n",
    "#     'warmup_step' : warmup_step.state_dict()\n",
    "#     'scheduler' : scheduler.state_dict()\n",
    "    \n",
    "# }, PATH + 'ver6.tar')  # 여러 가지 값 저장, 학습 중 진행 상황 저장을 위해 epoch, loss 값 등 일반 scalar값 저장 가능\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f027f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성값 예측하는 함수 만들기\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 위에서 설정한 tok, max_len, batch_size, device를 그대로 입력\n",
    "# comment : 예측하고자 하는 텍스트 데이터 리스트\n",
    "def getSentimentValue(comment, tok, max_len, batch_size, device):\n",
    "    commnetslist = [] # 텍스트 데이터를 담을 리스트\n",
    "    emo_list = [] # 감성 값을 담을 리스트\n",
    "    for c in comment: # 모든 댓글\n",
    "        commnetslist.append( [c, 5] ) # [댓글, 임의의 양의 정수값] 설정\n",
    "\n",
    "    pdData = pd.DataFrame( commnetslist, columns = [['뉴스', '감성']] )\n",
    "    pdData = pdData.values\n",
    "    test_set = BERTDataset(pdData, 0, 1, tok, max_len, True, False) \n",
    "    test_input = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_input):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length \n",
    "        # 이때, out이 예측 결과 리스트\n",
    "        \n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        print(out)\n",
    "        # e는 2가지 실수 값으로 구성된 리스트\n",
    "        # 0번 인덱스가 더 크면 부정, 긍정은 반대\n",
    "        for e in out:\n",
    "            if e[0]>e[1]: # 부정\n",
    "                value = 0\n",
    "                emo_list.append(\"부정\")\n",
    "                print('부정')\n",
    "            else: #긍정\n",
    "                value = 1\n",
    "                emo_list.append(\"긍정\")\n",
    "                print('긍정')\n",
    "                \n",
    "\n",
    "    return emo_list # 텍스트 데이터에 1대1 매칭되는 감성값 리스트 반환\n",
    "\n",
    "# input : 텍스트 데이터 리스트 외 KoBERT 설정 파라미터들\n",
    "# output : 입력한 텍스트 데이터 리스트와 1대 1 매칭 되는 감성 값 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf2dcd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스기사 테스트 함수\n",
    "def news():\n",
    "    \n",
    "    comment = []\n",
    "    comment.append(input(\"원하는 기사를 입력하세요\\n\\n\"))\n",
    "\n",
    "    for c in comment:\n",
    "        print(f'\\n기사 : {c}\\n')\n",
    "        \n",
    "    return getSentimentValue(comment, tok, max_len, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35ed7a35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원하는 기사를 입력하세요\n",
      "\n",
      "DD\n",
      "\n",
      "기사 : DD\n",
      "\n",
      "tensor([[ 4.4583, -3.8848]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "부정\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['부정']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ee1a33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c09e94cb66412183c78a21f6996026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fd534f62a088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch {} test acc {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-23f276147274>\u001b[0m in \u001b[0;36mcalc_accuracy\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmax_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_indices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval() # 평가 모드로 변경\n",
    "    \n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(PATH + 'model.pt')  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "model.load_state_dict(torch.load(PATH + 'model_state_dict.pt'))  # state_dict를 불러 온 후, 모델에 저장\n",
    "\n",
    "checkpoint = torch.load(PATH + 'all.tar')   # dict 불러오기\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 호출\n",
    "model.train()\n",
    "\n",
    "# 사용용 호출\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py36_pytorch",
   "language": "python",
   "name": "conda-env-azureml_py36_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
